# Supported Models

DeepMyst provides a unified API for accessing various language models with built-in token optimization. The platform supports models from multiple providers through a single, consistent interface.

## Available Models

DeepMyst currently supports the following models:

### OpenAI Models
- `gpt-4o-mini` - GPT-4o Mini
- `gpt-4o` - GPT-4o
- `o1` - OpenAI o1
- `o1-mini` - OpenAI o1-mini
- `o3-mini` - OpenAI o3-mini
- `chatgpt-4o-latest` - ChatGPT-4o Latest

### Anthropic Models
- `claude-3-7-sonnet-20250219` - Claude 3.7 Sonnet
- `claude-3-5-sonnet-latest` - Claude 3.5 Sonnet
- `claude-3-5-haiku-latest` - Claude 3 Haiku
- `claude-3-opus-latest` - Claude 3 Opus

### Google Models
- `gemini-2.0-flash` - Gemini 2.0 Flash
- `gemini-2.0-flash-lite-preview-02-05` - Gemini 2.0 Flash Lite
- `gemini-1.5-pro` - Gemini 1.5 Pro
- `gemini-1.5-flash` - Gemini 1.5 Flash
- `gemini-1.5-flash-8b` - Gemini 1.5 Flash 8B

### Groq Models
- `llama-3.1-8b-instant` - Llama 3.1 8B Instant
- `llama-3.3-70b-versatile` - Llama 3.3 70B Versatile
- `llama-guard-3-8b` - Llama Guard 3 8B
- `mixtral-8x7b-32768` - Mixtral 8x7B 32K
- `gemma2-9b-it` - Gemma2 9B IT
- `qwen-2.5-32b` - Qwen 2.5 32B
- `deepseek-r1-distill-qwen-32b` - Deepseek R1 Qwen 32B
- `deepseek-r1-distill-llama-70b-specdec` - Deepseek R1 Llama 70B Spec
- `deepseek-r1-distill-llama-70b` - Deepseek R1 Llama 70B

## Using Models with Direct API Requests

### Standard Request

```javascript
// Standard request with Claude 3.7 Sonnet
const response = await fetch('https://api.deepmyst.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer YOUR_API_KEY`
  },
  body: JSON.stringify({
    model: 'claude-3-7-sonnet-20250219',
    messages: [
      { role: 'user', content: 'What are the benefits of token optimization?' }
    ]
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

### Optimized Request

```javascript
// Using token optimization with GPT-4o
const response = await fetch('https://api.deepmyst.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer YOUR_API_KEY`
  },
  body: JSON.stringify({
    model: 'gpt-4o-optimize', // Note the -optimize suffix
    messages: [
      { role: 'user', content: 'Explain quantum computing in simple terms.' }
    ]
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

### Streaming Request

```javascript
// Streaming response with Gemini 1.5 Pro
const response = await fetch('https://api.deepmyst.com/v1/chat/completions', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer YOUR_API_KEY`
  },
  body: JSON.stringify({
    model: 'gemini-1.5-pro-optimize',
    messages: [
      { role: 'user', content: 'Write a short story about AI.' }
    ],
    stream: true
  })
});

// Process the stream
const reader = response.body.getReader();
const decoder = new TextDecoder('utf-8');

while (true) {
  const { done, value } = await reader.read();
  if (done) break;
  
  const chunk = decoder.decode(value);
  const lines = chunk.split('\n').filter(line => line.trim() !== '' && line.trim() !== 'data: [DONE]');
  
  for (const line of lines) {
    if (line.startsWith('data: ')) {
      const jsonStr = line.slice(6);
      
      try {
        const parsed = JSON.parse(jsonStr);
        const content = parsed.choices?.[0]?.delta?.content || '';
        if (content) process.stdout.write(content);
      } catch (e) {
        console.error('Error parsing chunk:', e);
      }
    }
  }
}
```

## Using Models with OpenAI Library

You can use the OpenAI SDK with DeepMyst by simply changing the base URL. This allows you to leverage familiar OpenAI patterns while accessing all supported models.

### Installation

```bash
npm install openai
```

### Configuration

```javascript
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: 'YOUR_API_KEY',
  baseURL: 'https://api.deepmyst.com/v1'
});
```

### Standard Request

```javascript
// Using Llama 3.3 70B through OpenAI SDK
async function generateResponse() {
  const completion = await openai.chat.completions.create({
    model: 'llama-3.3-70b-versatile',
    messages: [
      { role: 'user', content: 'Compare and contrast different AI architectures.' }
    ]
  });

  console.log(completion.choices[0].message.content);
}

generateResponse();
```

### Optimized Request

```javascript
// Using Mixtral 8x7B with optimization
async function generateOptimizedResponse() {
  const completion = await openai.chat.completions.create({
    model: 'mixtral-8x7b-32768-optimize', // Note the -optimize suffix
    messages: [
      { role: 'system', content: 'You are a helpful assistant.' },
      { role: 'user', content: 'Explain how transformers work in machine learning.' }
    ]
  });

  console.log(completion.choices[0].message.content);
}

generateOptimizedResponse();
```

### Streaming Request

```javascript
// Streaming with OpenAI SDK using Claude 3 Opus
async function generateStreamingResponse() {
  const stream = await openai.chat.completions.create({
    model: 'claude-3-opus-latest-optimize',
    messages: [
      { role: 'user', content: 'Write a poem about artificial intelligence.' }
    ],
    stream: true
  });

  // Process the stream
  for await (const chunk of stream) {
    const content = chunk.choices[0]?.delta?.content || '';
    if (content) process.stdout.write(content);
  }
}

generateStreamingResponse();
```

## Model Selection Guidance

- Use `-optimize` suffix when token efficiency is important
- Choose smaller models (mini variants) for faster responses and lower costs
- Choose larger models (opus, pro variants) for more complex reasoning tasks
- For high-throughput applications, consider models like `llama-3.1-8b-instant` or `gemini-1.5-flash`
- Consider using the router to automatically route to the best model
